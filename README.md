# PCLIM: Prototype-Based Category-Level Image–Text Multimodal Learning Method for Fetal Cardiac Ultrasound Imaging Analysis

## [Abstract]
Image–text multimodal learning in artificial intelligence–based fetal cardiac ultrasound imaging analysis is hindered by instance-level alignment. Moreover, instance-level alignment performs poorly in scenarios with scarce annotations and noisy text, which has greatly limited the development of this field. To address this, this paper proposes Prototype-based Category-Level Image–Text Multimodal Learning (PCLIM), which integrates: (1) Prototype-based Labeled Image–Text Contrastive Learning that refines image–text features via Self \& Cross-Attention Fusion and optimizes prototype pools through prototype contrastive loss; (2) Dynamic multi-prototype representation using dual‑modality prototype pools updated online via EMA and memory buffers to capture evolving intra‑class semantics; and (3) Prototype-matched Unlabeled Multimodal Learning that employs an EMA teacher–student framework to match unlabeled samples to category‑level pseudo‑text anchors with consistency regularization and contrastive learning. Experiments on fetal cardiac ultrasound datasets EP-CHDT8000 and SYF-CHD30000 demonstrate that PCLIM consistently outperforms state‑of‑the‑art baselines, especially in low‑label scenarios.
